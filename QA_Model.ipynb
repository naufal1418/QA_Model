{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2023 CITS4012 Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "\n",
        "**Installing Packages**\n",
        "\n",
        "Some basic package installations are needed for some modules to be imported.\n",
        "### List of packages used (version):\n",
        "nltk == 3.8.1  \n",
        "re == 2.2.1  \n",
        "torch == 2.0.1+cu118  \n",
        "numpy == 1.22.4  \n",
        "pandas == 1.5.3  \n",
        "en_core_web_sm == 3.5.0\n",
        "\n",
        "**File Locations**\n",
        "\n",
        "The default file locations are given as './Data/WikiQA-train.tsv' and './Data/WikiQA-test.tsv'. Need to make the necessary change in the file locations as per the requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: joblib in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (8.1.2)\n",
            "Requirement already satisfied: tqdm in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: spacy in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (62.1.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/runpy.py\", line 144, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/runpy.py\", line 111, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/spacy/errors.py\", line 2, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/spacy/compat.py\", line 3, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/thinc/config.py\", line 4, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/thinc/types.py\", line 8, in <module>\n",
            "    from .compat import has_cupy, cupy\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/thinc/compat.py\", line 54, in <module>\n",
            "    import tensorflow.experimental.dlpack\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n",
            "    from tensorflow.python.eager import context\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 32, in <module>\n",
            "    from tensorflow.core.framework import function_pb2\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py\", line 14, in <module>\n",
            "    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 14, in <module>\n",
            "    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 14, in <module>\n",
            "    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 14, in <module>\n",
            "    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 36, in <module>\n",
            "    _descriptor.FieldDescriptor(\n",
            "  File \"/Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages/google/protobuf/descriptor.py\", line 561, in __new__\n",
            "    _message.Message._CheckCalledFromGeneratedFile()\n",
            "TypeError: Descriptors cannot not be created directly.\n",
            "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
            "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
            " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
            " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
            "\n",
            "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n"
          ]
        }
      ],
      "source": [
        "# Installing spacy for nltk\n",
        "!pip install nltk\n",
        "\n",
        "# Installing spacy for Named Entity Tagging\n",
        "!pip install spacy\n",
        "\n",
        "# Downloading the pre-trained NLP Model for Named Entity Tagging\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To overrie the error while installing en_core_web_sm\n",
        "import os\n",
        "\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# For Named Enity Tagging\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from collections import Counter\n",
        "\n",
        "# For Modelling\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enabling GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to format the Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shrinkColumns(df):\n",
        "    '''Function to convert the given data to the required format'''\n",
        "    new_df = pd.DataFrame(columns=['QuestionID', 'Question', 'Document', 'Answer'])\n",
        "\n",
        "    for qid in df['QuestionID'].unique():\n",
        "        # Get the first question associated with this QuestionID\n",
        "        first_question = df.loc[df['QuestionID'] == qid, 'Question'].iloc[0]\n",
        "        \n",
        "        # Get all sentences associated with this QuestionID\n",
        "        sentences = df.loc[df['QuestionID'] == qid, 'Sentence']\n",
        "        concatenated_sentence = ' '.join(sentences)\n",
        "        \n",
        "        # Get the sentence associated with this QuestionID where the Label is 1\n",
        "        answer = df.loc[(df['QuestionID'] == qid) & (df['Label'] == 1), 'Sentence']\n",
        "\n",
        "        if not answer.empty:\n",
        "            answer = answer.iloc[0]\n",
        "        else:\n",
        "            answer = \"\"\n",
        "        \n",
        "        # Add the QuestionID, first_question, concatenated_sentence, and answer to the new dataframe\n",
        "        new_row = {'QuestionID': qid, 'Question': first_question, 'Document': concatenated_sentence, 'Answer': answer}\n",
        "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for labelling the document tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generateLabels(padded_document, answer, len_org_document):\n",
        "    '''The Function generates three labels: [Answer], [Not Answer] and [PAD], depending on the position of\n",
        "        the answer in the document'''\n",
        "    labels = [\"[Not Answer]\" for i in range(len(padded_document))]\n",
        "\n",
        "    # Generationg the labels for all the documents with non emply answers\n",
        "    if answer != \"\":\n",
        "        start_index = [i for i in range(len(padded_document)-len(answer)+1) if padded_document[i:i+len(answer)] == answer]\n",
        "        if start_index:\n",
        "            start_index = start_index[0]\n",
        "            end_index = start_index + len(answer)\n",
        "            labels[start_index] = '[Answer]'\n",
        "            for j in range(start_index+1, end_index):\n",
        "                labels[j] = '[Answer]'\n",
        "            labels[end_index-1] = '[Answer]'\n",
        "            \n",
        "    # labelling the padding\n",
        "    for i in range(len_org_document, len(padded_document)):\n",
        "        labels[i] = \"[Pad]\"\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for tokenising a sentance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(sentance):\n",
        "    '''Function for tokenising a sentance'''\n",
        "    sent_text=[]\n",
        "    normalized_text = []\n",
        "    sent_text.extend(word_tokenize(sentance))\n",
        "    \n",
        "    # Removing punctuation and changing all characters to lower case\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9.]+\", '', string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "\n",
        "    final_text = []\n",
        "    for text in normalized_text:\n",
        "        if text != '':\n",
        "            final_text.append(text)\n",
        "\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for Tokenising a list of sentances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenizeList(sequences):\n",
        "    '''Function for tokenising a list of sentance or a document'''\n",
        "    tokenized_list = []\n",
        "    for seq in sequences:\n",
        "        tokenized_list.append(tokenize(seq))\n",
        "\n",
        "    return tokenized_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for word embedding a sentance (Using Word2Vec - Skip Gram Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2Vec(sentance):\n",
        "    '''Function to convert a sentance from words to vector (of size 50), using Skip Gram method'''\n",
        "    wv_sg_model = Word2Vec([sentance], vector_size=50, window=3, min_count=1, workers=2, sg=1)\n",
        "\n",
        "    word_2_vec = list()\n",
        "\n",
        "    # Vectorising each word of the sentance\n",
        "    for word in sentance:\n",
        "        word_2_vec.append(wv_sg_model.wv[word])\n",
        "    return word_2_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for word embedding a document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2VecDocuments(document):\n",
        "    '''Function to vectorise the words of sentances in a document'''\n",
        "    word_2_vec = list()\n",
        "    for sentance in document:\n",
        "        word_2_vec.append(word2Vec(sentance))\n",
        "    return word_2_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to add padding to the sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def padSequences(sequences, length):\n",
        "    '''Function for padding the sequences to the length supplied'''\n",
        "    padded_sequences = list()\n",
        "\n",
        "    for seq in sequences:\n",
        "        num_padding = length - len(seq)\n",
        "        padded_seq = seq + ['[PAD]'] * num_padding\n",
        "        padded_sequences.append(padded_seq)\n",
        "    \n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to find the TF-IDF values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tfIdf(tokens, length):\n",
        "    '''Function for finding the TF-IDF Values of a word in the document'''\n",
        "    tf_idf_list = list()\n",
        "    DF = {}\n",
        "    for term in np.unique(tokens):\n",
        "        try:\n",
        "            DF[term] +=1\n",
        "        except:\n",
        "            DF[term] =1\n",
        "\n",
        "    tf_idf = []\n",
        "    N = len(tokens) \n",
        "    doc_id = 0\n",
        "    counter = Counter(tokens)\n",
        "    total_num_words = len(tokens) \n",
        "\n",
        "    # Calculationg the TF-IDF value for the words\n",
        "    for term in tokens[0:length]:\n",
        "        tf = counter[term]/total_num_words\n",
        "        df = DF[term]\n",
        "        idf = math.log(N/(df+1))+1\n",
        "        tf_idf.append(tf*idf)\n",
        "\n",
        "    # Giving the TF-IDF values for padding as '0'\n",
        "    for term in range(length,len(tokens)):\n",
        "        tf_idf.append(0)\n",
        "\n",
        "    doc_id += 1\n",
        "    tf_idf_list.append(tf_idf)\n",
        "\n",
        "    return tf_idf_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def posTagging(tokens, length):\n",
        "    '''Function for calculating the POS tags of words in the documents'''\n",
        "    tagged_words = pos_tag(tokens[0:length])\n",
        "    _, tags_list = zip(*tagged_words)\n",
        "    tags_list = list(tags_list)\n",
        "\n",
        "    # Tagging [PAD] as [PAD]\n",
        "    for i in range(length,len(tokens)):\n",
        "        tags_list.append('[PAD]')\n",
        "    return tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to find the Named Entity Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nerTagging(document):\n",
        "    '''Function for calculating the Named Entity tags for Words in the document '''\n",
        "    NE_Tag_table = []\n",
        "    tokens = []\n",
        "    \n",
        "    # loading pre-trained model of NER\n",
        "    entity_tagging_model = en_core_web_sm.load()\n",
        "\n",
        "    article = entity_tagging_model(document)\n",
        "    sentences = [x for x in article.sents]\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            NE_Tag_table.append(str(word.ent_type_))\n",
        "            tokens.append(str(word).lower())\n",
        "    for i in range(len(NE_Tag_table)):\n",
        "        if(NE_Tag_table[i] == ''):\n",
        "            NE_Tag_table[i] = \"O\"\n",
        "\n",
        "    return tokens, NE_Tag_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get the wordnet POS tag and convert to use with lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getWordnetPos(tags):\n",
        "    '''Function to get the Wordnet POS tags which is used to lemmettize the words'''\n",
        "    if tags.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tags.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tags.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif tags.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to Lemmattize the words using the POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatization(tokens, tags):\n",
        "    '''Function to lemmettize the words considering the POS tags'''\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmitized = [lemmatizer.lemmatize(tokens[ind], pos=getWordnetPos(tags[ind])) for ind in range(len(tokens))]  \n",
        "    return lemmitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to Preprocess the Questions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def questionPreprocess(question_list):\n",
        "    '''Function to pre-process the questions'''\n",
        "    question_tokens = tokenizeList(question_list)\n",
        "\n",
        "    MAX_LENGTH = max([len(s) for s in question_tokens])\n",
        "    \n",
        "    #length of 97% of the data\n",
        "    length = round(0.7*MAX_LENGTH)\n",
        "\n",
        "    question_tokens_padded = padSequences(question_tokens, MAX_LENGTH)\n",
        "    embedded_question_list = word2VecDocuments(question_tokens_padded)\n",
        "    question_batch_torch = torch.from_numpy(np.array(embedded_question_list)).float().to(device)\n",
        "    return question_batch_torch, MAX_LENGTH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getNERTags(document_list):\n",
        "    '''Function to find the NER tags of the document'''\n",
        "    document_tokens = []\n",
        "    NER_tags = []\n",
        "\n",
        "    for document in document_list:\n",
        "        tokens, tags = nerTagging(document)\n",
        "        document_tokens.append(tokens)\n",
        "        NER_tags.append(tags)\n",
        "\n",
        "    MAX_LENGTH = max([len(s) for s in document_tokens])\n",
        "\n",
        "    # NER Tags for the padding\n",
        "    for ind in range(len(document_list)):\n",
        "        tag_len = len(NER_tags[ind])\n",
        "        for i in range(tag_len,MAX_LENGTH):\n",
        "            NER_tags[ind].append(\"[PAD]\")\n",
        "\n",
        "    return document_tokens, NER_tags\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def documentPreprocessing(document_list, option):\n",
        "    '''Function to pre-process the list of documents and return a torch vector'''\n",
        "    \n",
        "    document_tokens, ner_tags = getNERTags(document_list)\n",
        "\n",
        "    MAX_LENGTH = max([len(s) for s in document_tokens])\n",
        "\n",
        "    # padding documents with maximum length\n",
        "    document_tokens_padded = padSequences(document_tokens, MAX_LENGTH)\n",
        "\n",
        "    # Word Embeddings\n",
        "    pos_tags = []\n",
        "    lem_document_tokens = []\n",
        "    tf_idf = []\n",
        "    for ind in range(len(document_tokens_padded)):\n",
        "        len_org_document = len(document_tokens[ind])\n",
        "        # POS tagging\n",
        "        tags = posTagging(document_tokens_padded[ind],len_org_document)\n",
        "        pos_tags.append(tags)\n",
        "\n",
        "        # Lemmatization\n",
        "        lem_document_tokens.append(lemmatization(document_tokens_padded[ind],tags))\n",
        "\n",
        "        # TF-IDF\n",
        "        tf_idf.append(tfIdf(lem_document_tokens[ind],len_org_document))\n",
        "\n",
        "    # Word to Vector\n",
        "    embedded_document_list = word2VecDocuments(lem_document_tokens)\n",
        "    embedded_pos_tags = word2VecDocuments(pos_tags)\n",
        "    embedded_NER_tags = word2VecDocuments(ner_tags)\n",
        "\n",
        "    document_vector = []\n",
        "    if(option == 1):\n",
        "        # Just Word2Vec\n",
        "        for i in range(len(embedded_document_list)):\n",
        "            embedded_document = embedded_document_list[i]\n",
        "            \n",
        "            token_vector =  []\n",
        "            for j in range(len(embedded_document)):\n",
        "                vector = []\n",
        "                vector.extend(embedded_document[j])\n",
        "                token_vector.append(np.array(vector))\n",
        "            document_vector.append(token_vector)\n",
        "    elif(option == 2):\n",
        "        # word2Vec, pos tags\n",
        "        for i in range(len(embedded_document_list)):\n",
        "            embedded_document = embedded_document_list[i]\n",
        "            embedded_pos = embedded_pos_tags[i]\n",
        "            \n",
        "            token_vector =  []\n",
        "            for j in range(len(embedded_document)):\n",
        "                vector = []\n",
        "                vector.extend(embedded_document[j])\n",
        "                vector.extend(embedded_pos[j])\n",
        "                token_vector.append(np.array(vector))\n",
        "            document_vector.append(token_vector)\n",
        "    elif(option == 3):\n",
        "        # word2Vec, pos tagging, NER tagging and TF-IDF\n",
        "        for i in range(len(embedded_document_list)):\n",
        "            embedded_document = embedded_document_list[i]\n",
        "            embedded_pos = embedded_pos_tags[i]\n",
        "            embedded_NER = embedded_NER_tags[i]\n",
        "            tfidf = tf_idf[i]\n",
        "            \n",
        "            token_vector =  []\n",
        "            for j in range(len(embedded_document)):\n",
        "                vector = []\n",
        "                vector.extend(embedded_document[j])\n",
        "                vector.extend(embedded_pos[j])\n",
        "                vector.extend(embedded_NER[j])\n",
        "                vector.append(tfidf[j])\n",
        "                token_vector.append(np.array(vector))\n",
        "            document_vector.append(token_vector)\n",
        "\n",
        "    document_vector_torch = torch.from_numpy(np.array(document_vector)).float().to(device)\n",
        "    return document_tokens_padded, document_vector_torch, MAX_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Helper functions for displaying the time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def asMinutes(s):\n",
        "    '''Function for converting the seconds into minutes'''\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    '''Sunction to find the estimated and the remaining time'''\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# Reading the data from the local disk\n",
        "training_data = pd.read_csv('./Data/WikiQA-train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('./Data/WikiQA-test.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Formatting the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Formatting the data as per the requirments\n",
        "formatted_training_data = shrinkColumns(training_data)\n",
        "formatted_test_data = shrinkColumns(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Question Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "# Getting the question list from the main dataframe\n",
        "question_list = formatted_training_data[\"Question\"]\n",
        "\n",
        "# Converting the question list to torch vectors\n",
        "question_vector_torch, MAX_LEN_Q = questionPreprocess(question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Document Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the document list from the main dataframe\n",
        "document_list = formatted_training_data[\"Document\"]\n",
        "\n",
        "# document embedding - option values\n",
        "# 1 - only word embedding\n",
        "# 2 - word embedding and pos tagging\n",
        "# 3 - word embedding, pos tagging, NER tagging and TF-IDF\n",
        "option = 3\n",
        "padded_document_tokens, document_vector_torch, MAX_LEN_D = documentPreprocessing(document_list, option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Label Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the list of answers from the dataframe\n",
        "answer_list = formatted_training_data[\"Answer\"]\n",
        "answer_tokens = tokenizeList(answer_list)\n",
        "\n",
        "# Generating Labels\n",
        "document_labels = list()\n",
        "for ind in range(len(padded_document_tokens)):\n",
        "    len_org_document = len(padded_document_tokens[ind])\n",
        "    document_labels.append(generateLabels(padded_document_tokens[ind], answer_tokens[ind],len_org_document))\n",
        "\n",
        "# Word to vector\n",
        "embedded_document_labels = word2VecDocuments(document_labels)\n",
        "\n",
        "# To torch vectors\n",
        "label_vector_torch = torch.from_numpy(np.array(embedded_document_labels)).float().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.QA Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Summary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class question_Summary(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(question_Summary, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Bi-directional RNN\n",
        "        self.rnn = nn.RNN(self.input_size, self.hidden_size, batch_first =True, bidirectional=True)\n",
        "\n",
        "    def forward(self, input):     \n",
        "        _, h_n = self.rnn(input)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0:],h_n[1:]),0)\n",
        "        return hidden_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class doc_Attention(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\" \n",
        "    ATTN_TYPE_COSINE_SIMILARITY = \"Cosine Similarity\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(doc_Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        \n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, 2*self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*4, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, question_summary, method):\n",
        "        # Dot Product Attention\n",
        "        if method == doc_Attention.ATTN_TYPE_DOT_PRODUCT:\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, question_summary.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, question_summary.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "        # Scaled Dot Product Attention\n",
        "        elif method == doc_Attention.ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, question_summary.T.unsqueeze(0)), dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, question_summary.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0].detach(), hidden[0].detach()), 1) / np.sqrt(hidden_size)\n",
        "        # Cosine Similarity Attention\n",
        "        elif method == doc_Attention.ATTN_TYPE_COSINE_SIMILARITY:\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, question_summary.T.unsqueeze(0)), dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, question_summary.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0].detach(), hidden[0].detach()), 1) / (norm(attn_output[0].detach())*norm(hidden[0].detach()))\n",
        "\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, question_summary, method ):\n",
        "        _, hidden = self.gru(input, hidden)\n",
        "\n",
        "        concat_output = self.cal_attention(hidden, question_summary, method)\n",
        "\n",
        "        output = F.softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(MAX_LEN_D, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(input_question_tensor, input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, method = doc_Attention.ATTN_TYPE_DOT_PRODUCT):\n",
        "    \n",
        "    input_length = input_question_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    hidden_size = input_tensor.shape[2]\n",
        "\n",
        "    question_summary = torch.zeros(MAX_LEN_Q, hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_hidden = encoder(input_question_tensor[i])\n",
        "        question_summary[i] = encoder_hidden[0,0]\n",
        "\n",
        "    # it is for storing the hidden states of input sequence later, which will be used for calculating the attention during the decoding process\n",
        "    decoder_hidden = torch.zeros(1, MAX_LEN_D, hidden_size*2, device=device)\n",
        "\n",
        "    decoder_input = input_tensor\n",
        "\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for i in range(target_length):\n",
        "        decoder_output, _ = decoder(decoder_input, decoder_hidden, question_summary, method)\n",
        "        target = target_tensor[i]\n",
        "        for j in range(len(target_tensor[i])):\n",
        "            loss += criterion(decoder_output[j], target[j]) \n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "def trainIters(encoder, decoder, input_tensor, question_tensor, label_tensor, n_iters, method , print_every=200, plot_every=200, learning_rate=0.002):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    input_question_tensor = question_tensor\n",
        "    input_tensor = input_tensor\n",
        "    target_tensor = label_tensor\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        loss = train(input_question_tensor, input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, method)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Parameter Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attention - option values\n",
        "# 1 - Dot Product\n",
        "# 2 - Scalar Dot Product\n",
        "# 3 - Scalar Dot Product\n",
        "attention_option = 1\n",
        "\n",
        "if(attention_option == 1):\n",
        "    attn_method = doc_Attention.ATTN_TYPE_DOT_PRODUCT\n",
        "elif(attention_option == 2):\n",
        "    attn_method = doc_Attention.ATTN_TYPE_SCALE_DOT_PRODUCT\n",
        "elif(attention_option == 3):\n",
        "    attn_method = doc_Attention.ATTN_TYPE_COSINE_SIMILARITY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor = document_vector_torch\n",
        "question_tensor = question_vector_torch\n",
        "label_tensor = label_vector_torch\n",
        "\n",
        "hidden_size = input_tensor.shape[2]\n",
        "learning_rate = 1e-3\n",
        "input_size = question_tensor.shape[2]\n",
        "output_size = question_tensor.shape[2]\n",
        "n_iterations = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0m 6s (- 0m 27s) (2 20%) -89.4510\n",
            "0m 13s (- 0m 20s) (4 40%) -90.8810\n",
            "0m 20s (- 0m 13s) (6 60%) -107.9134\n",
            "0m 27s (- 0m 6s) (8 80%) -115.3991\n",
            "0m 33s (- 0m 0s) (10 100%) -115.4147\n"
          ]
        }
      ],
      "source": [
        "quest_summ_model = question_Summary(input_size, hidden_size).to(device)\n",
        "doc_attn_model = doc_Attention(hidden_size, output_size, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(quest_summ_model, doc_attn_model, input_tensor, question_tensor, label_tensor, n_iterations, attn_method, print_every=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(quest_summ_model, 'question_summary_model.pt')\n",
        "torch.save(doc_attn_model, 'document_attention_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Model Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Function to Evaluate the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, questions, documents_torch, MAX_LEN_Q, MAX_LEN_D, attn_option):\n",
        "    '''Function to evaluate the models'''\n",
        "    encoder.eval() # Turn on the evaluation mode\n",
        "    decoder.eval() # Turn on the evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if(attn_option == 1):\n",
        "            attn_method = doc_Attention.ATTN_TYPE_DOT_PRODUCT\n",
        "        elif(attn_option == 2):\n",
        "            attn_method = doc_Attention.ATTN_TYPE_SCALE_DOT_PRODUCT\n",
        "        elif(attn_option == 3):\n",
        "            attn_method = doc_Attention.ATTN_TYPE_COSINE_SIMILARITY\n",
        "\n",
        "\n",
        "        input_length = MAX_LEN_Q\n",
        "\n",
        "        question_summary = torch.zeros(MAX_LEN_Q, hidden_size*2, device=device)\n",
        "\n",
        "        decoder_hidden = torch.zeros(1, MAX_LEN_D, hidden_size*2, device=device)\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_hidden = encoder(questions[i])\n",
        "            question_summary[i] = encoder_hidden[0,0]\n",
        "\n",
        "        decoder_input = documents_torch\n",
        "\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, question_summary, attn_method)\n",
        "        topv, topi = decoder_output.data.topk(1) # simply adopt the predicted tag with the highest probabiity\n",
        "\n",
        "        return topi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Helper function to get the sentences with Predicted words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSentances(documents, words):\n",
        "    '''Function to predict the sentances based on the predicted words'''\n",
        "    sentences_with_word = []\n",
        "\n",
        "    tokenizer = nltk.sent_tokenize\n",
        "    \n",
        "    for ind in range(len(documents)):\n",
        "        sentences = tokenizer(documents[ind])\n",
        "        word = words[ind]\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            if word in sentence:\n",
        "                sentences_with_word.append(sentence)\n",
        "    \n",
        "    return sentences_with_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Function to Predict the sentances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prediction(pred_ind, doc_list, ans_list):\n",
        "    '''Function to predict the sentances from the main document list'''\n",
        "\n",
        "    predicted_words = [] \n",
        "\n",
        "    doc_tokens = tokenizeList(doc_list)\n",
        "    for i in range(len(doc_tokens)):\n",
        "        document = doc_tokens[i]\n",
        "        if(len(document) > pred_ind[i]):\n",
        "            predicted_words.append(document[pred_ind[i].item()])\n",
        "        else:\n",
        "            predicted_words.append(\"\")\n",
        "\n",
        "\n",
        "    predicted_sentances = getSentances(doc_list, predicted_words)\n",
        "    \n",
        "    target_sentences = ans_list\n",
        "\n",
        "    total_sentences = len(doc_list)\n",
        "    correct_sentences = [\"\"]*total_sentences\n",
        "\n",
        "    for ind in range(len(target_sentences)):\n",
        "        for j in range(len(predicted_sentances)):\n",
        "            if target_sentences[ind] == predicted_sentances[j]:\n",
        "                correct_sentences[ind] = predicted_sentances[j]\n",
        "\n",
        "    #accuracy = correct_sentences / total_sentences\n",
        "\n",
        "    return target_sentences, correct_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Function to Print the accuracy Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def printScores(report, decimal_places = 2):\n",
        "    '''Function to print the scores from the classification report'''\n",
        "\n",
        "    accuracy = round(report['accuracy'], decimal_places)\n",
        "    macro_avg_precision = round(report['macro avg']['precision'], decimal_places)\n",
        "    macro_avg_recall = round(report['macro avg']['recall'], decimal_places)\n",
        "    macro_avg_f1_score = round(report['macro avg']['f1-score'], decimal_places)\n",
        "    weighted_avg_precision = round(report['weighted avg']['precision'], decimal_places)\n",
        "    weighted_avg_recall = round(report['weighted avg']['recall'], decimal_places)\n",
        "    weighted_avg_f1_score = round(report['weighted avg']['f1-score'], decimal_places)\n",
        "\n",
        "    print(f\"Accuracy: \\t\\t\\t{accuracy}\")\n",
        "    print(f\"Macro Average Precision: \\t{macro_avg_precision}\")\n",
        "    print(f\"Macro Average Recall: \\t\\t{macro_avg_recall}\")\n",
        "    print(f\"Macro Average F1-score: \\t{macro_avg_f1_score}\")\n",
        "    print(f\"Weighted Average Precision: \\t{weighted_avg_precision}\")\n",
        "    print(f\"Weighted Average Recall: \\t{weighted_avg_recall}\")\n",
        "    print(f\"Weighted Average F1-score: \\t{weighted_avg_f1_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pre-processing the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the question list from the main dataframe\n",
        "test_question_list = formatted_test_data[\"Question\"]\n",
        "\n",
        "# Converting the question list to torch vectors\n",
        "test_question_vector_torch, T_MAX_LEN_Q = questionPreprocess(test_question_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the document list from the main dataframe\n",
        "test_document_list = formatted_test_data[\"Document\"]\n",
        "\n",
        "option = 3\n",
        "_, test_document_vector_torch, T_MAX_LEN_D = documentPreprocessing(test_document_list, option)\n",
        "\n",
        "# Getting the list of answers from the dataframe\n",
        "test_answer_list = formatted_test_data[\"Answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: \t\t\t0.77\n",
            "Macro Average Precision: \t1.0\n",
            "Macro Average Recall: \t\t0.4\n",
            "Macro Average F1-score: \t0.4\n",
            "Weighted Average Precision: \t0.83\n",
            "Weighted Average Recall: \t0.77\n",
            "Weighted Average F1-score: \t0.67\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the Model\n",
        "\n",
        "predicted_indexes = evaluate(quest_summ_model, doc_attn_model, test_question_vector_torch, test_document_vector_torch, T_MAX_LEN_Q, T_MAX_LEN_D, 1)\n",
        "target_sentences, predicted_sentences = prediction(predicted_indexes, test_document_list, test_answer_list)\n",
        "report = classification_report(target_sentences, predicted_sentences, output_dict=True, zero_division=1)\n",
        "\n",
        "printScores(report, decimal_places = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNGfO0h9I3W"
      },
      "source": [
        "###3.1. Input Embedding Ablation Study(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVsyvrc9VHL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Embedding\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.8\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.5\n",
            "Macro Average F1-score: \t0.48\n",
            "Weighted Average Precision: \t0.85\n",
            "Weighted Average Recall: \t0.8\n",
            "Weighted Average F1-score: \t0.71\n",
            "\n",
            "Word Embedding + POS Tags\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.85\n",
            "Macro Average Precision: \t0.98\n",
            "Macro Average Recall: \t\t0.62\n",
            "Macro Average F1-score: \t0.61\n",
            "Weighted Average Precision: \t0.88\n",
            "Weighted Average Recall: \t0.85\n",
            "Weighted Average F1-score: \t0.78\n",
            "\n",
            "Word Embedding + POS Tags + NER Tags + TF-IDF\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.75\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.38\n",
            "Macro Average F1-score: \t0.35\n",
            "Weighted Average Precision: \t0.82\n",
            "Weighted Average Recall: \t0.75\n",
            "Weighted Average F1-score: \t0.65\n",
            "\n"
          ]
        }
      ],
      "source": [
        "embedding =    [\"Word Embedding\",\n",
        "                \"Word Embedding + POS Tags\",\n",
        "                \"Word Embedding + POS Tags + NER Tags + TF-IDF\"]\n",
        "\n",
        "for embedding_option in range(1,4):\n",
        "\n",
        "    #Training\n",
        "    _, document_vector_torch, MAX_LEN_D = documentPreprocessing(document_list, embedding_option)\n",
        "\n",
        "    input_tensor = document_vector_torch\n",
        "    question_tensor = question_vector_torch\n",
        "    label_tensor = label_vector_torch\n",
        "\n",
        "    hidden_size = input_tensor.shape[2]\n",
        "    input_size = question_tensor.shape[2]\n",
        "    output_size = question_tensor.shape[2]\n",
        "    n_iterations = 10\n",
        "\n",
        "    quest_summ_model = question_Summary(input_size, hidden_size).to(device)\n",
        "    doc_attn_model = doc_Attention(hidden_size, output_size, dropout_p=0.1).to(device)\n",
        "\n",
        "    trainIters(quest_summ_model, doc_attn_model, input_tensor, question_tensor, label_tensor, n_iterations, 1)\n",
        "    \n",
        "\n",
        "    #Testing\n",
        "    _, test_document_vector_torch, T_MAX_LEN_D = documentPreprocessing(test_document_list, embedding_option)\n",
        "\n",
        "    predicted_indexes = evaluate(quest_summ_model, doc_attn_model, test_question_vector_torch, test_document_vector_torch, T_MAX_LEN_Q, T_MAX_LEN_D, 1)\n",
        "    target_sentences, predicted_sentences = prediction(predicted_indexes, test_document_list, test_answer_list)\n",
        "    report = classification_report(target_sentences, predicted_sentences, output_dict=True, zero_division=1)\n",
        "\n",
        "    print(embedding[embedding_option - 1])\n",
        "    print(\"-------------------------------\")\n",
        "    printScores(report, decimal_places = 2)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7nFwMo9WBE"
      },
      "source": [
        "###3.2. Attention Ablation Study\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRK-BeiNSVi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 - Dot Product\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.8\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.5\n",
            "Macro Average F1-score: \t0.48\n",
            "Weighted Average Precision: \t0.85\n",
            "Weighted Average Recall: \t0.8\n",
            "Weighted Average F1-score: \t0.71\n",
            "\n",
            "2 - Scalar Dot Product\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.75\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.38\n",
            "Macro Average F1-score: \t0.35\n",
            "Weighted Average Precision: \t0.82\n",
            "Weighted Average Recall: \t0.75\n",
            "Weighted Average F1-score: \t0.65\n",
            "\n",
            "3 - Cosine Similarity\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.8\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.5\n",
            "Macro Average F1-score: \t0.48\n",
            "Weighted Average Precision: \t0.85\n",
            "Weighted Average Recall: \t0.8\n",
            "Weighted Average F1-score: \t0.71\n",
            "\n"
          ]
        }
      ],
      "source": [
        "attention =[\"1 - Dot Product\", \n",
        "            \"2 - Scalar Dot Product\",\n",
        "            \"3 - Cosine Similarity\"]\n",
        "for attention_option in range(1,4):\n",
        "\n",
        "    if(attention_option == 1):\n",
        "        attention_method = doc_Attention.ATTN_TYPE_DOT_PRODUCT\n",
        "    elif(attention_option == 2):\n",
        "        attention_method = doc_Attention.ATTN_TYPE_SCALE_DOT_PRODUCT\n",
        "    elif(attention_option == 3):\n",
        "        attention_method = doc_Attention.ATTN_TYPE_COSINE_SIMILARITY\n",
        "\n",
        "    input_tensor = document_vector_torch\n",
        "    question_tensor = question_vector_torch\n",
        "    label_tensor = label_vector_torch\n",
        "\n",
        "    hidden_size = input_tensor.shape[2]\n",
        "    input_size = question_tensor.shape[2]\n",
        "    output_size = question_tensor.shape[2]\n",
        "    n_iterations = 10\n",
        "\n",
        "    quest_summ_model = question_Summary(input_size, hidden_size).to(device)\n",
        "    doc_attn_model = doc_Attention(hidden_size, output_size, dropout_p=0.1).to(device)\n",
        "\n",
        "    trainIters(quest_summ_model, doc_attn_model, input_tensor, question_tensor, label_tensor, n_iterations, attention_method)\n",
        "\n",
        "    \n",
        "    predicted_indexes = evaluate(quest_summ_model, doc_attn_model, test_question_vector_torch, test_document_vector_torch, T_MAX_LEN_Q, T_MAX_LEN_D, attention_option)\n",
        "    target_sentences, predicted_sentences = prediction(predicted_indexes, test_document_list, test_answer_list)\n",
        "    report = classification_report(target_sentences, predicted_sentences, output_dict=True, zero_division=1)\n",
        "\n",
        "    print(attention[attention_option - 1])\n",
        "    print(\"-------------------------------\")\n",
        "    printScores(report, decimal_places = 2)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzGjUe6NDnB"
      },
      "source": [
        "###3.3. Hyper Parameter Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xj4PNyrNDBH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning Rate =  0.01\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.75\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.38\n",
            "Macro Average F1-score: \t0.35\n",
            "Weighted Average Precision: \t0.82\n",
            "Weighted Average Recall: \t0.75\n",
            "Weighted Average F1-score: \t0.65\n",
            "\n",
            "Learning Rate =  0.001\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.75\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.38\n",
            "Macro Average F1-score: \t0.35\n",
            "Weighted Average Precision: \t0.82\n",
            "Weighted Average Recall: \t0.75\n",
            "Weighted Average F1-score: \t0.65\n",
            "\n",
            "Learning Rate =  0.002\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.75\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.38\n",
            "Macro Average F1-score: \t0.35\n",
            "Weighted Average Precision: \t0.82\n",
            "Weighted Average Recall: \t0.75\n",
            "Weighted Average F1-score: \t0.65\n",
            "\n",
            "Learning Rate =  0.0001\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.8\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.5\n",
            "Macro Average F1-score: \t0.48\n",
            "Weighted Average Precision: \t0.85\n",
            "Weighted Average Recall: \t0.8\n",
            "Weighted Average F1-score: \t0.71\n",
            "\n",
            "Learning Rate =  1e-05\n",
            "-------------------------------\n",
            "Accuracy: \t\t\t0.75\n",
            "Macro Average Precision: \t0.97\n",
            "Macro Average Recall: \t\t0.38\n",
            "Macro Average F1-score: \t0.35\n",
            "Weighted Average Precision: \t0.82\n",
            "Weighted Average Recall: \t0.75\n",
            "Weighted Average F1-score: \t0.65\n",
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rates =[1e-2, 1e-3, 2e-3, 1e-4, 1e-5]\n",
        "for learning_rate in learning_rates:\n",
        "\n",
        "    input_tensor = document_vector_torch\n",
        "    question_tensor = question_vector_torch\n",
        "    label_tensor = label_vector_torch\n",
        "\n",
        "    hidden_size = input_tensor.shape[2]\n",
        "    input_size = question_tensor.shape[2]\n",
        "    output_size = question_tensor.shape[2]\n",
        "    n_iterations = 10\n",
        "\n",
        "    quest_summ_model = question_Summary(input_size, hidden_size).to(device)\n",
        "    doc_attn_model = doc_Attention(hidden_size, output_size, dropout_p=0.1).to(device)\n",
        "\n",
        "    trainIters(quest_summ_model, doc_attn_model, input_tensor, question_tensor, label_tensor, n_iterations,learning_rate = learning_rate, method = doc_Attention.ATTN_TYPE_DOT_PRODUCT)\n",
        "\n",
        "    \n",
        "    predicted_indexes = evaluate(quest_summ_model, doc_attn_model, test_question_vector_torch, test_document_vector_torch, T_MAX_LEN_Q, T_MAX_LEN_D, attention_option)\n",
        "    target_sentences, predicted_sentences = prediction(predicted_indexes, test_document_list, test_answer_list)\n",
        "    report = classification_report(target_sentences, predicted_sentences, output_dict=True, zero_division=1)\n",
        "\n",
        "    print(\"Learning Rate = \",learning_rate)\n",
        "    print(\"-------------------------------\")\n",
        "    printScores(report, decimal_places = 2)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
